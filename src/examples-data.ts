/**
 * AUTO-GENERATED FILE - DO NOT EDIT DIRECTLY
 * Generated by scripts/build-examples.ts from examples/*.yaml
 * Run 'npm run build:examples' to regenerate.
 */

import type { PipelineExample } from './examples-registry';

export const PIPELINE_EXAMPLES: PipelineExample[] = [
  {
    "id": "amqp-0-9-consumer",
    "name": "AMQP 0.9 Consumer (RabbitMQ)",
    "description": "Consume messages from RabbitMQ using AMQP 0.9.1 protocol",
    "keywords": [
      "amqp",
      "rabbitmq",
      "queue",
      "consumer",
      "input",
      "messaging",
      "0.9"
    ],
    "components": {
      "inputs": [
        "amqp_0_9"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  amqp_0_9:\n    urls:\n      - amqp://guest:guest@localhost:5672/\n    queue: my-queue\n    consumer_tag: my-consumer\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.received_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "amqp-0-9-producer",
    "name": "AMQP 0.9 Producer (RabbitMQ)",
    "description": "Publish messages to RabbitMQ using AMQP 0.9.1 protocol",
    "keywords": [
      "amqp",
      "rabbitmq",
      "queue",
      "producer",
      "output",
      "messaging",
      "publish",
      "0.9"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "amqp_0_9"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.published_at = now()\n\noutput:\n  amqp_0_9:\n    urls:\n      - amqp://guest:guest@localhost:5672/\n    exchange: my-exchange\n    key: my-routing-key"
  },
  {
    "id": "amqp-1-consumer",
    "name": "AMQP 1.0 Consumer",
    "description": "Consume messages using AMQP 1.0 protocol",
    "keywords": [
      "amqp",
      "queue",
      "consumer",
      "input",
      "messaging",
      "1.0",
      "azure",
      "service",
      "bus"
    ],
    "components": {
      "inputs": [
        "amqp_1"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  amqp_1:\n    url: amqp://localhost:5672/\n    source_address: /my-queue\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.received_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "amqp-1-producer",
    "name": "AMQP 1.0 Producer",
    "description": "Publish messages using AMQP 1.0 protocol",
    "keywords": [
      "amqp",
      "queue",
      "producer",
      "output",
      "messaging",
      "publish",
      "1.0",
      "azure",
      "service",
      "bus"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "amqp_1"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.published_at = now()\n\noutput:\n  amqp_1:\n    url: amqp://localhost:5672/\n    target_address: /my-queue"
  },
  {
    "id": "api-gateway",
    "name": "API Gateway Pipeline",
    "description": "HTTP server that receives requests and forwards to backend",
    "keywords": [
      "api",
      "gateway",
      "http",
      "server",
      "webhook",
      "proxy"
    ],
    "components": {
      "inputs": [
        "http_server"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "http_client"
      ]
    },
    "bloblangPatterns": [
      "now()"
    ],
    "yaml": "input:\n  http_server:\n    address: 0.0.0.0:8080\n    path: /api\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.received_at = now()\n\noutput:\n  http_client:\n    url: https://backend.example.com/process\n    verb: POST"
  },
  {
    "id": "archive-squash",
    "name": "Archive and Squash Messages",
    "description": "Combine multiple messages into a single JSON payload",
    "keywords": [
      "archive",
      "squash",
      "combine",
      "merge",
      "aggregate",
      "opc-ua"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping",
        "archive"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "archive format: json_array",
      "append()",
      "squash()"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics: [opc-ua-tags]\n\npipeline:\n  processors:\n    - mapping: |\n        let field_name = match {\n          meta(\"opcua_node_id\") == \"ns=4;i=3\" => \"machineNumber\",\n          meta(\"opcua_node_id\") == \"ns=4;i=4\" => \"dataSetNumber\",\n          _ => meta(\"opcua_tag_name\")\n        }\n        root = {\n          $field_name: this.value\n        }\n    - archive:\n        format: json_array\n    - mapping: |\n        root = this\n          .append({\"timestamp_ms\": (timestamp_unix_nano() / 1000000).floor()})\n          .squash()\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: combined-opc-ua"
  },
  {
    "id": "array-average",
    "name": "Array Average Calculation",
    "description": "Calculate average of numerical values in an array",
    "keywords": [
      "array",
      "average",
      "sum",
      "length",
      "calculate",
      "aggregate"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "sum()",
      "length()",
      "min()",
      "max()"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics: [measurements]\n\npipeline:\n  processors:\n    - mapping: |\n        let values = this.measurements\n        let total = $values.sum()\n        let count = $values.length()\n        let average = $total / $count\n        root = {\n          \"timestamp_ms\": (timestamp_unix_nano() / 1000000).floor(),\n          \"average_measurement\": $average,\n          \"min\": $values.min(),\n          \"max\": $values.max()\n        }\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: statistics"
  },
  {
    "id": "avro-decoder",
    "name": "Avro Decoder Pipeline",
    "description": "Decode Avro messages and convert to JSON",
    "keywords": [
      "avro",
      "decode",
      "decoder",
      "schema",
      "confluent",
      "kafka"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "schema_registry_decode",
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - avro-events\n\npipeline:\n  processors:\n    - schema_registry_decode:\n        url: http://localhost:8081\n    - mapping: |\n        root = this\n        root.decoded_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "aws-dynamodb-output",
    "name": "AWS DynamoDB Writer",
    "description": "Write data to AWS DynamoDB table",
    "keywords": [
      "aws",
      "dynamodb",
      "write",
      "database",
      "nosql",
      "put",
      "item"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "aws_dynamodb"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - users\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.updated_at = now()\n\noutput:\n  aws_dynamodb:\n    table: users\n    region: us-east-1\n    string_columns:\n      id: ${! this.id }\n      name: ${! this.name }\n      email: ${! this.email }"
  },
  {
    "id": "aws-kinesis-firehose",
    "name": "AWS Kinesis Firehose Delivery",
    "description": "Deliver data to AWS Kinesis Firehose for S3, Redshift, or Elasticsearch",
    "keywords": [
      "aws",
      "kinesis",
      "firehose",
      "delivery",
      "s3",
      "redshift",
      "elasticsearch",
      "streaming"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "aws_kinesis_firehose"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n    batching:\n      count: 100\n      period: 10s\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.delivered_at = now()\n\noutput:\n  aws_kinesis_firehose:\n    stream: my-firehose-stream\n    region: us-east-1"
  },
  {
    "id": "aws-kinesis-output",
    "name": "AWS Kinesis Stream Output",
    "description": "Write messages to AWS Kinesis Data Streams",
    "keywords": [
      "aws",
      "kinesis",
      "stream",
      "output",
      "write",
      "realtime",
      "streaming"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "aws_kinesis"
      ]
    },
    "bloblangPatterns": [
      "parse_json()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.partition_key = this.user_id\n\noutput:\n  aws_kinesis:\n    stream: my-kinesis-stream\n    partition_key: ${! this.partition_key }\n    region: us-east-1"
  },
  {
    "id": "aws-lambda-processor",
    "name": "AWS Lambda Processor",
    "description": "Process messages through AWS Lambda functions",
    "keywords": [
      "aws",
      "lambda",
      "processor",
      "serverless",
      "function",
      "transform"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "aws_lambda",
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "parse_json()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - raw-events\n\npipeline:\n  processors:\n    - aws_lambda:\n        function: my-processing-function\n        region: us-east-1\n    - mapping: |\n        root = this.parse_json()\n        root.processed_by = \"lambda\"\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: processed-events"
  },
  {
    "id": "aws-sns-publish",
    "name": "AWS SNS Publisher",
    "description": "Publish messages to AWS SNS topics for fan-out messaging",
    "keywords": [
      "aws",
      "sns",
      "publish",
      "notify",
      "fanout",
      "pubsub",
      "notification",
      "topic"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "aws_sns"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.published_at = now()\n\noutput:\n  aws_sns:\n    topic_arn: arn:aws:sns:us-east-1:123456789012:my-topic\n    region: us-east-1"
  },
  {
    "id": "aws-sqs-to-sns-fanout",
    "name": "SQS to SNS Fan-Out",
    "description": "Read from SQS and publish to multiple SNS topics",
    "keywords": [
      "aws",
      "sqs",
      "sns",
      "fanout",
      "queue",
      "pubsub",
      "broadcast"
    ],
    "components": {
      "inputs": [
        "aws_sqs"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "broker",
        "aws_sns"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  aws_sqs:\n    url: https://sqs.us-east-1.amazonaws.com/123456789012/my-queue\n    region: us-east-1\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.distributed_at = now()\n\noutput:\n  broker:\n    pattern: fan_out\n    outputs:\n      - aws_sns:\n          topic_arn: arn:aws:sns:us-east-1:123456789012:notifications\n          region: us-east-1\n      - aws_sns:\n          topic_arn: arn:aws:sns:us-east-1:123456789012:analytics\n          region: us-east-1"
  },
  {
    "id": "azure-blob-storage-input",
    "name": "Azure Blob Storage Input",
    "description": "Read files from Azure Blob Storage containers",
    "keywords": [
      "azure",
      "blob",
      "storage",
      "input",
      "read",
      "container",
      "files",
      "microsoft"
    ],
    "components": {
      "inputs": [
        "azure_blob_storage"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "meta()",
      "now()",
      "env()"
    ],
    "yaml": "input:\n  azure_blob_storage:\n    storage_account: mystorageaccount\n    storage_access_key: ${! env(\"AZURE_STORAGE_KEY\") }\n    container: my-container\n    prefix: incoming/\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.blob_name = meta(\"blob_storage_key\")\n        root.processed_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "azure-blob-storage-output",
    "name": "Azure Blob Storage Output",
    "description": "Write data to Azure Blob Storage containers",
    "keywords": [
      "azure",
      "blob",
      "storage",
      "output",
      "write",
      "container",
      "upload",
      "microsoft"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "azure_blob_storage"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()",
      "format_timestamp()",
      "uuid_v4()",
      "env()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.archived_at = now()\n\noutput:\n  azure_blob_storage:\n    storage_account: mystorageaccount\n    storage_access_key: ${! env(\"AZURE_STORAGE_KEY\") }\n    container: archive\n    path: ${! now().format_timestamp(\"2006/01/02\") }/${! uuid_v4() }.json"
  },
  {
    "id": "azure-cosmosdb-input",
    "name": "Azure CosmosDB Input",
    "description": "Read documents from Azure CosmosDB",
    "keywords": [
      "azure",
      "cosmosdb",
      "cosmos",
      "database",
      "nosql",
      "input",
      "read",
      "microsoft"
    ],
    "components": {
      "inputs": [
        "azure_cosmosdb"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "now()",
      "env()"
    ],
    "yaml": "input:\n  azure_cosmosdb:\n    endpoint: https://mycosmosaccount.documents.azure.com:443\n    account_key: ${! env(\"COSMOSDB_KEY\") }\n    database: mydb\n    container: events\n    partition_keys_map: root = this.id\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.read_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "azure-cosmosdb-output",
    "name": "Azure CosmosDB Output",
    "description": "Write documents to Azure CosmosDB",
    "keywords": [
      "azure",
      "cosmosdb",
      "cosmos",
      "database",
      "nosql",
      "output",
      "write",
      "microsoft"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "azure_cosmosdb"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()",
      "env()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.id = this.event_id\n        root.stored_at = now()\n\noutput:\n  azure_cosmosdb:\n    endpoint: https://mycosmosaccount.documents.azure.com:443\n    account_key: ${! env(\"COSMOSDB_KEY\") }\n    database: mydb\n    container: events\n    partition_keys_map: root = this.id\n    operation: create"
  },
  {
    "id": "azure-queue-storage-input",
    "name": "Azure Queue Storage Input",
    "description": "Consume messages from Azure Queue Storage",
    "keywords": [
      "azure",
      "queue",
      "storage",
      "input",
      "consume",
      "messages",
      "microsoft"
    ],
    "components": {
      "inputs": [
        "azure_queue_storage"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()",
      "env()"
    ],
    "yaml": "input:\n  azure_queue_storage:\n    storage_account: mystorageaccount\n    storage_access_key: ${! env(\"AZURE_STORAGE_KEY\") }\n    queue_name: my-queue\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.received_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "azure-queue-storage-output",
    "name": "Azure Queue Storage Output",
    "description": "Send messages to Azure Queue Storage",
    "keywords": [
      "azure",
      "queue",
      "storage",
      "output",
      "send",
      "messages",
      "microsoft"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "azure_queue_storage"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()",
      "env()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.queued_at = now()\n\noutput:\n  azure_queue_storage:\n    storage_account: mystorageaccount\n    storage_access_key: ${! env(\"AZURE_STORAGE_KEY\") }\n    queue_name: processed-events"
  },
  {
    "id": "azure-table-storage-input",
    "name": "Azure Table Storage Input",
    "description": "Read entities from Azure Table Storage",
    "keywords": [
      "azure",
      "table",
      "storage",
      "input",
      "read",
      "entities",
      "microsoft"
    ],
    "components": {
      "inputs": [
        "azure_table_storage"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "now()",
      "env()"
    ],
    "yaml": "input:\n  azure_table_storage:\n    storage_account: mystorageaccount\n    storage_access_key: ${! env(\"AZURE_STORAGE_KEY\") }\n    table_name: mytable\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.read_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "azure-table-storage-output",
    "name": "Azure Table Storage Output",
    "description": "Write entities to Azure Table Storage",
    "keywords": [
      "azure",
      "table",
      "storage",
      "output",
      "write",
      "entities",
      "microsoft"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "azure_table_storage"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()",
      "env()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.PartitionKey = this.category\n        root.RowKey = this.id\n        root.stored_at = now()\n\noutput:\n  azure_table_storage:\n    storage_account: mystorageaccount\n    storage_access_key: ${! env(\"AZURE_STORAGE_KEY\") }\n    table_name: events"
  },
  {
    "id": "base64-decode",
    "name": "Base64 Decoding",
    "description": "Decode base64-encoded values in messages",
    "keywords": [
      "base64",
      "decode",
      "encode",
      "binary",
      "transform"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "decode(\"base64\")"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics: [encoded-data]\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.decoded_value = this.encoded_value.decode(\"base64\")\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: decoded-data"
  },
  {
    "id": "basic-example",
    "name": "Basic Pipeline Example",
    "description": "A basic example pipeline that reads, processes, and outputs data",
    "keywords": [
      "basic",
      "example",
      "simple",
      "starter",
      "template",
      "demo"
    ],
    "components": {
      "inputs": [
        "generate"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "uuid_v4()",
      "now()"
    ],
    "yaml": "input:\n  generate:\n    count: 10\n    interval: 1s\n    mapping: |\n      root.id = uuid_v4()\n      root.timestamp = now()\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.processed = true\n\noutput:\n  stdout: {}"
  },
  {
    "id": "batch-aggregation",
    "name": "Batch Aggregation",
    "description": "Aggregate messages into batches with time windows",
    "keywords": [
      "batch",
      "aggregate",
      "window",
      "time",
      "group"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "aws_s3"
      ]
    },
    "bloblangPatterns": [
      "uuid_v4()",
      "map_each()",
      "parse_json()",
      "length()",
      "now()",
      "format_timestamp()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n    batching:\n      count: 100\n      period: 10s\n\npipeline:\n  processors:\n    - mapping: |\n        root.batch_id = uuid_v4()\n        root.events = this.map_each(e -> e.parse_json())\n        root.count = this.length()\n        root.timestamp = now()\n\noutput:\n  aws_s3:\n    bucket: event-batches\n    path: ${! now().format_timestamp(\"2006/01/02\") }/${! this.batch_id }.json"
  },
  {
    "id": "batch-window-processing",
    "name": "Batch Window Processing",
    "description": "Process messages in time-based or count-based windows",
    "keywords": [
      "batch",
      "window",
      "time",
      "count",
      "aggregate",
      "group",
      "buffer",
      "collect"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "aws_s3"
      ]
    },
    "bloblangPatterns": [
      "map_each()",
      "parse_json()",
      "uuid_v4()",
      "length()",
      "index()",
      "now()",
      "format_timestamp()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n    batching:\n      count: 1000\n      period: 60s\n      processors:\n        - mapping: |\n            root = this.map_each(e -> e.parse_json())\n\npipeline:\n  processors:\n    - mapping: |\n        root.window_id = uuid_v4()\n        root.events = this\n        root.event_count = this.length()\n        root.window_start = this.index(0).timestamp\n        root.window_end = now()\n\noutput:\n  aws_s3:\n    bucket: event-windows\n    path: ${! now().format_timestamp(\"2006/01/02/15\") }/${! this.window_id }.json"
  },
  {
    "id": "bedrock-summarization",
    "name": "AWS Bedrock Summarization",
    "description": "Summarize documents using AWS Bedrock Claude",
    "keywords": [
      "bedrock",
      "aws",
      "summarize",
      "summary",
      "claude",
      "ai",
      "ml",
      "document",
      "text"
    ],
    "components": {
      "inputs": [
        "aws_s3"
      ],
      "processors": [
        "aws_bedrock_chat",
        "mapping"
      ],
      "outputs": [
        "aws_s3"
      ]
    },
    "bloblangPatterns": [
      "index()",
      "now()"
    ],
    "yaml": "input:\n  aws_s3:\n    bucket: documents\n    prefix: raw/\n\npipeline:\n  processors:\n    - mapping: |\n        root.messages = [\n          {\n            \"role\": \"user\",\n            \"content\": \"Summarize the following document in 3 bullet points:\\n\\n\" + this\n          }\n        ]\n    - aws_bedrock_chat:\n        model: anthropic.claude-3-haiku-20240307-v1:0\n        region: us-east-1\n    - mapping: |\n        root.original_key = this.key\n        root.summary = this.choices.index(0).message.content\n        root.summarized_at = now()\n\noutput:\n  aws_s3:\n    bucket: documents\n    path: summaries/${! this.original_key }.json"
  },
  {
    "id": "bitmask-logic",
    "name": "Bitmask Status Decoding",
    "description": "Decode status bits from integer values",
    "keywords": [
      "bitmask",
      "bit",
      "status",
      "decode",
      "binary",
      "flag"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "int64()",
      "modulo %",
      "integer division"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics: [machine-status]\n\npipeline:\n  processors:\n    - mapping: |\n        let status_code = this.status.number().round().int64()\n        root = {\n          \"status_code\": status_code,\n          \"machine_running\": ($status_code % 2) == 1,\n          \"error_detected\": (($status_code / 2) % 2) == 1,\n          \"maintenance_required\": (($status_code / 4) % 2) == 1,\n          \"timestamp_ms\": (timestamp_unix_nano() / 1000000).floor()\n        }\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: decoded-status"
  },
  {
    "id": "cloudwatch-logs",
    "name": "AWS CloudWatch Logs",
    "description": "Send logs to AWS CloudWatch Logs",
    "keywords": [
      "cloudwatch",
      "logs",
      "aws",
      "logging",
      "monitoring"
    ],
    "components": {
      "inputs": [
        "stdin"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "aws_cloudwatch_logs"
      ]
    },
    "bloblangPatterns": [
      "now()",
      "ts_unix_milli()",
      "env()"
    ],
    "yaml": "input:\n  stdin:\n    codec: lines\n\npipeline:\n  processors:\n    - mapping: |\n        root.message = this\n        root.timestamp = now().ts_unix_milli()\n\noutput:\n  aws_cloudwatch_logs:\n    log_group: /my-app/logs\n    log_stream: ${! env(\"HOSTNAME\") }\n    region: us-east-1"
  },
  {
    "id": "cockroachdb-changefeed",
    "name": "CockroachDB Changefeed",
    "description": "Process CockroachDB changefeed events",
    "keywords": [
      "cockroachdb",
      "changefeed",
      "cdc",
      "distributed",
      "sql",
      "streaming",
      "change"
    ],
    "components": {
      "inputs": [
        "cockroachdb_changefeed"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "string()",
      "if { } else { }"
    ],
    "yaml": "input:\n  cockroachdb_changefeed:\n    dsn: postgres://root@localhost:26257/mydb\n    tables:\n      - transactions\n    cursor_cache: file://./cursor.txt\n\npipeline:\n  processors:\n    - mapping: |\n        root.id = this.key.string()\n        root.payload = this.value\n        root.timestamp = this.updated.string()\n        root.operation = if this.value == null { \"delete\" } else { \"upsert\" }\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: transaction-changes"
  },
  {
    "id": "conditional-error-detection",
    "name": "Conditional Error Detection",
    "description": "Detect errors using conditional logic (bit relations)",
    "keywords": [
      "conditional",
      "error",
      "detect",
      "boolean",
      "logic",
      "state"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "== comparison",
      "&& and",
      "|| or"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics: [machine-state]\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.Error = this.State == \"run\" && this.speed == 0\n        root.is_idle = this.State == \"idle\" || this.speed < 10\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: state-analysis"
  },
  {
    "id": "connection-pooling",
    "name": "HTTP Connection Pooling",
    "description": "Configure connection pooling for HTTP outputs",
    "keywords": [
      "connection",
      "pool",
      "pooling",
      "http",
      "performance",
      "concurrent",
      "optimization"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "http_client"
      ]
    },
    "bloblangPatterns": [
      "parse_json()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n\noutput:\n  http_client:\n    url: https://api.example.com/ingest\n    verb: POST\n    headers:\n      Content-Type: application/json\n    max_in_flight: 64\n    batching:\n      count: 100\n      period: 1s\n    timeout: 30s\n    retry_period: 1s\n    max_retry_backoff: 30s\n    retries: 3"
  },
  {
    "id": "content-based-router",
    "name": "Content-Based Router",
    "description": "Route messages based on content fields using switch",
    "keywords": [
      "router",
      "routing",
      "switch",
      "content",
      "based",
      "conditional",
      "branch",
      "dispatch"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "switch"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "let",
      "meta"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - mapping: |\n        let event = this.parse_json()\n        root = $event\n        meta event_type = $event.type\n\noutput:\n  switch:\n    cases:\n      - check: meta(\"event_type\") == \"order\"\n        output:\n          kafka:\n            addresses:\n              - localhost:9092\n            topic: orders\n      - check: meta(\"event_type\") == \"payment\"\n        output:\n          kafka:\n            addresses:\n              - localhost:9092\n            topic: payments\n      - check: meta(\"event_type\") == \"shipment\"\n        output:\n          kafka:\n            addresses:\n              - localhost:9092\n            topic: shipments\n      - output:\n          kafka:\n            addresses:\n              - localhost:9092\n            topic: other-events"
  },
  {
    "id": "content-moderation",
    "name": "AI Content Moderation",
    "description": "Moderate content using OpenAI moderation API",
    "keywords": [
      "moderation",
      "content",
      "safety",
      "ai",
      "openai",
      "filter",
      "toxic",
      "harmful",
      "compliance"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "http",
        "mapping"
      ],
      "outputs": [
        "switch"
      ]
    },
    "bloblangPatterns": [
      "env()",
      "parse_json()",
      "index()",
      "if { } else { }",
      "meta"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - user-content\n\npipeline:\n  processors:\n    - mapping: |\n        root.input = this.content\n    - http:\n        url: https://api.openai.com/v1/moderations\n        verb: POST\n        headers:\n          Authorization: Bearer ${! env(\"OPENAI_API_KEY\") }\n          Content-Type: application/json\n    - mapping: |\n        let result = this.parse_json()\n        root.content_id = this.content_id\n        root.content = this.content\n        root.flagged = $result.results.index(0).flagged\n        root.categories = $result.results.index(0).categories\n        meta flagged = if $result.results.index(0).flagged { \"true\" } else { \"false\" }\n\noutput:\n  switch:\n    cases:\n      - check: meta(\"flagged\") == \"true\"\n        output:\n          kafka:\n            addresses:\n              - localhost:9092\n            topic: flagged-content\n      - output:\n          kafka:\n            addresses:\n              - localhost:9092\n            topic: approved-content"
  },
  {
    "id": "csv-to-json",
    "name": "CSV to JSON Converter",
    "description": "Read CSV files and convert to JSON format",
    "keywords": [
      "csv",
      "json",
      "convert",
      "converter",
      "parse",
      "transform",
      "file"
    ],
    "components": {
      "inputs": [
        "file"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "now()"
    ],
    "yaml": "input:\n  file:\n    paths:\n      - /data/*.csv\n    codec: csv\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.converted_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "data-enrichment",
    "name": "Data Enrichment Pipeline",
    "description": "Enrich data with additional information",
    "keywords": [
      "data",
      "enrichment",
      "enrich",
      "enhance",
      "augment",
      "lookup"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping",
        "http"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - raw-data\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n    - http:\n        url: http://enrichment-service/lookup\n        verb: POST\n    - mapping: |\n        root = this.parse_json()\n        root.enriched_at = now()\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: enriched-data"
  },
  {
    "id": "data-masking",
    "name": "Data Masking Pipeline",
    "description": "Mask sensitive data fields for privacy",
    "keywords": [
      "data",
      "masking",
      "mask",
      "pii",
      "privacy",
      "redact",
      "sensitive"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "re_replace_all()",
      "slice()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - raw-data\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.email = this.email.re_replace_all(\"(.).*@\", \"$1***@\")\n        root.phone = \"***-***-\" + this.phone.slice(-4)\n        root.ssn = \"***-**-\" + this.ssn.slice(-4)\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: masked-data"
  },
  {
    "id": "datetime-conversion",
    "name": "DateTime Conversion (UTC to Local)",
    "description": "Convert timestamps between UTC and local time formats",
    "keywords": [
      "datetime",
      "timestamp",
      "utc",
      "local",
      "timezone",
      "convert",
      "format"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "ts_parse()",
      "ts_format()"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics: [events]\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.local_time = this.utc_time.ts_parse(\"2006-01-02T15:04:05Z\").ts_format(\"2006-01-02 15:04:05\", \"Local\")\n        root.formatted = this.timestamp.ts_parse(\"RFC3339\").ts_format(\"Jan 02, 2006 3:04 PM\")\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: formatted-events"
  },
  {
    "id": "dead-letter-queue",
    "name": "Dead Letter Queue Pattern",
    "description": "Route failed messages to a dead letter queue",
    "keywords": [
      "error",
      "dlq",
      "dead",
      "letter",
      "queue",
      "retry",
      "fallback"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "try",
        "catch",
        "mapping"
      ],
      "outputs": [
        "switch"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()",
      "error()",
      "meta()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - incoming\n\npipeline:\n  processors:\n    - try:\n        - mapping: |\n            root = this.parse_json()\n            root.processed_at = now()\n    - catch:\n        - mapping: |\n            root.original = this\n            root.error = error()\n            root.failed_at = now()\n            meta failed = \"true\"\n\noutput:\n  switch:\n    cases:\n      - check: meta(\"failed\") == \"true\"\n        output:\n          kafka:\n            addresses:\n              - localhost:9092\n            topic: dead-letter-queue\n      - output:\n          kafka:\n            addresses:\n              - localhost:9092\n            topic: processed"
  },
  {
    "id": "dynamic-routing",
    "name": "Dynamic Output Selection",
    "description": "Dynamically select output based on message content",
    "keywords": [
      "dynamic",
      "routing",
      "output",
      "select",
      "variable",
      "destination",
      "runtime"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "dynamic"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "let",
      "match",
      "meta"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - mapping: |\n        let event = this.parse_json()\n        root = $event\n        # Set dynamic output based on region\n        meta output_target = match $event.region {\n          \"us\" => \"kafka_us\"\n          \"eu\" => \"kafka_eu\"\n          \"asia\" => \"kafka_asia\"\n          _ => \"kafka_default\"\n        }\n\noutput:\n  dynamic:\n    outputs:\n      kafka_us:\n        kafka:\n          addresses:\n            - us-kafka:9092\n          topic: regional-events\n      kafka_eu:\n        kafka:\n          addresses:\n            - eu-kafka:9092\n          topic: regional-events\n      kafka_asia:\n        kafka:\n          addresses:\n            - asia-kafka:9092\n          topic: regional-events\n      kafka_default:\n        kafka:\n          addresses:\n            - localhost:9092\n          topic: unrouted-events"
  },
  {
    "id": "dynamodb-to-s3",
    "name": "DynamoDB to S3",
    "description": "Scan DynamoDB table and write to S3",
    "keywords": [
      "dynamodb",
      "s3",
      "aws",
      "database",
      "backup",
      "export"
    ],
    "components": {
      "inputs": [
        "aws_dynamodb"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "aws_s3"
      ]
    },
    "bloblangPatterns": [
      "now()",
      "timestamp_unix()"
    ],
    "yaml": "input:\n  aws_dynamodb:\n    table: my-table\n    region: us-east-1\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.exported_at = now()\n\noutput:\n  aws_s3:\n    bucket: my-backup-bucket\n    path: dynamodb/${! timestamp_unix() }.json"
  },
  {
    "id": "endianness-swap",
    "name": "Hex Endianness Swap",
    "description": "Handle byte order conversion for Modbus or binary data",
    "keywords": [
      "endian",
      "endianness",
      "hex",
      "binary",
      "modbus",
      "byte",
      "swap",
      "reverse"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "decode(\"hex\")",
      "reverse()",
      "encode(\"hex\")"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics: [modbus-data]\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.correct_endian = this.hex_value.decode(\"hex\").reverse().encode(\"hex\")\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: corrected-data"
  },
  {
    "id": "fallback-chain",
    "name": "Fallback Output Chain",
    "description": "Try outputs in order until one succeeds",
    "keywords": [
      "fallback",
      "chain",
      "backup",
      "redundant",
      "failover",
      "resilience",
      "output"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "fallback"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()",
      "uuid_v4()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - critical-events\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.processed_at = now()\n\noutput:\n  fallback:\n    - http_client:\n        url: https://primary-api.example.com/ingest\n        verb: POST\n        retries: 3\n    - http_client:\n        url: https://secondary-api.example.com/ingest\n        verb: POST\n        retries: 3\n    - aws_s3:\n        bucket: fallback-storage\n        path: failed/${! uuid_v4() }.json"
  },
  {
    "id": "fan-out-fan-in",
    "name": "Fan-Out Fan-In Pattern",
    "description": "Split message, process branches in parallel, merge results",
    "keywords": [
      "parallel",
      "fan",
      "out",
      "in",
      "branch",
      "merge",
      "concurrent",
      "split",
      "workflow"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "branch",
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - orders\n\npipeline:\n  processors:\n    - branch:\n        request_map: 'root = this'\n        processors:\n          - mapping: |\n              root.enriched_user = {\"lookup\": \"user\", \"id\": this.user_id}\n        result_map: 'root.user_data = this.enriched_user'\n    - branch:\n        request_map: 'root = this'\n        processors:\n          - mapping: |\n              root.enriched_product = {\"lookup\": \"product\", \"id\": this.product_id}\n        result_map: 'root.product_data = this.enriched_product'\n    - mapping: |\n        root = this\n        root.enriched_at = now()\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: enriched-orders"
  },
  {
    "id": "field-extraction",
    "name": "Field Extraction Pipeline",
    "description": "Extract and transform specific fields from JSON data",
    "keywords": [
      "field",
      "extraction",
      "extract",
      "fields",
      "select",
      "transform",
      "mapping"
    ],
    "components": {
      "inputs": [
        "stdin"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "now()"
    ],
    "yaml": "input:\n  stdin: {}\n\npipeline:\n  processors:\n    - mapping: |\n        root.user_id = this.id\n        root.name = this.user.name\n        root.email = this.user.email\n        root.extracted_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "gcp-bigquery-select",
    "name": "GCP BigQuery Select",
    "description": "Query data from Google BigQuery tables",
    "keywords": [
      "gcp",
      "google",
      "bigquery",
      "query",
      "select",
      "sql",
      "input",
      "warehouse"
    ],
    "components": {
      "inputs": [
        "gcp_bigquery_select"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "now()"
    ],
    "yaml": "input:\n  gcp_bigquery_select:\n    project: my-project\n    table: mydataset.events\n    columns:\n      - id\n      - name\n      - timestamp\n    where: timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.queried_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "gcp-bigquery-write",
    "name": "GCP BigQuery Write API",
    "description": "Stream data to Google BigQuery using the Storage Write API",
    "keywords": [
      "gcp",
      "google",
      "bigquery",
      "write",
      "stream",
      "output",
      "warehouse",
      "insert"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "gcp_bigquery_write_api"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n    batching:\n      count: 100\n      period: 10s\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.loaded_at = now()\n\noutput:\n  gcp_bigquery_write_api:\n    project: my-project\n    dataset: mydataset\n    table: events"
  },
  {
    "id": "gcp-cloud-storage-input",
    "name": "GCP Cloud Storage Input",
    "description": "Read objects from Google Cloud Storage buckets",
    "keywords": [
      "gcp",
      "google",
      "cloud",
      "storage",
      "gcs",
      "bucket",
      "input",
      "read"
    ],
    "components": {
      "inputs": [
        "gcp_cloud_storage"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "meta()",
      "now()"
    ],
    "yaml": "input:\n  gcp_cloud_storage:\n    bucket: my-bucket\n    prefix: incoming/\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.object_name = meta(\"gcs_key\")\n        root.processed_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "gcp-cloud-storage-output",
    "name": "GCP Cloud Storage Output",
    "description": "Write objects to Google Cloud Storage buckets",
    "keywords": [
      "gcp",
      "google",
      "cloud",
      "storage",
      "gcs",
      "bucket",
      "output",
      "write"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "gcp_cloud_storage"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()",
      "format_timestamp()",
      "uuid_v4()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.archived_at = now()\n\noutput:\n  gcp_cloud_storage:\n    bucket: my-bucket\n    path: ${! now().format_timestamp(\"2006/01/02\") }/${! uuid_v4() }.json"
  },
  {
    "id": "gcp-pubsub-publish",
    "name": "GCP Pub/Sub Publisher",
    "description": "Publish messages to Google Cloud Pub/Sub topics",
    "keywords": [
      "gcp",
      "google",
      "pubsub",
      "publish",
      "output",
      "messaging",
      "topic"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "gcp_pubsub"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.published_at = now()\n\noutput:\n  gcp_pubsub:\n    project: my-project\n    topic: my-topic"
  },
  {
    "id": "gcp-pubsub-subscribe",
    "name": "GCP Pub/Sub Subscriber",
    "description": "Subscribe to Google Cloud Pub/Sub topics",
    "keywords": [
      "gcp",
      "google",
      "pubsub",
      "subscribe",
      "input",
      "messaging",
      "queue"
    ],
    "components": {
      "inputs": [
        "gcp_pubsub"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "meta()",
      "now()"
    ],
    "yaml": "input:\n  gcp_pubsub:\n    project: my-project\n    subscription: my-subscription\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.message_id = meta(\"gcp_pubsub_message_id\")\n        root.received_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "gcp-spanner-cdc",
    "name": "GCP Spanner Change Data Capture",
    "description": "Stream changes from Google Cloud Spanner using CDC",
    "keywords": [
      "gcp",
      "google",
      "spanner",
      "cdc",
      "change",
      "stream",
      "database",
      "realtime"
    ],
    "components": {
      "inputs": [
        "gcp_spanner_cdc"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [],
    "yaml": "input:\n  gcp_spanner_cdc:\n    project: my-project\n    instance: my-instance\n    database: my-database\n    stream: my-change-stream\n\npipeline:\n  processors:\n    - mapping: |\n        root.table = this.table_name\n        root.operation = this.mod_type\n        root.data = this.new_values\n        root.timestamp = this.commit_timestamp\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: spanner-changes"
  },
  {
    "id": "generate-random-data",
    "name": "Generate Random Test Data",
    "description": "Generate random test data with multiple transformations",
    "keywords": [
      "generate",
      "random",
      "test",
      "data",
      "stdout",
      "transform",
      "hash"
    ],
    "components": {
      "inputs": [
        "generate"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "uuid_v4()",
      "now()",
      "random_int()",
      "hash()",
      "encode()",
      "slice()"
    ],
    "yaml": "input:\n  generate:\n    count: 0\n    interval: 500ms\n    mapping: |\n      root.id = uuid_v4()\n\npipeline:\n  processors:\n    - mapping: |\n        root.id = this.id\n        root.created_at = now()\n        root.random_number = random_int(max: 1000)\n    - mapping: |\n        root.hash = root.id.hash(\"sha256\").encode(\"hex\").slice(0, 16)\n        root.is_even = root.random_number % 2 == 0\n\noutput:\n  stdout: {}"
  },
  {
    "id": "generate-uuid-stdout",
    "name": "Generate UUIDs to Stdout",
    "description": "Generate random UUIDs and print to standard output",
    "keywords": [
      "generate",
      "uuid",
      "stdout",
      "random",
      "test",
      "debug",
      "print",
      "console"
    ],
    "components": {
      "inputs": [
        "generate"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "uuid_v4()",
      "now()",
      "format_timestamp()",
      "ts_unix()"
    ],
    "yaml": "input:\n  generate:\n    count: 10\n    interval: 1s\n    mapping: |\n      root = {}\n\npipeline:\n  processors:\n    - mapping: |\n        root.uuid = uuid_v4()\n        root.timestamp = now()\n    - mapping: |\n        root.formatted = root.timestamp.format_timestamp(\"2006-01-02T15:04:05Z07:00\")\n        root.unix = root.timestamp.ts_unix()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "grok-log-parsing",
    "name": "Log Parsing with Grok",
    "description": "Parse log files using grok patterns",
    "keywords": [
      "grok",
      "log",
      "parsing",
      "parse",
      "regex",
      "pattern",
      "extract"
    ],
    "components": {
      "inputs": [
        "file"
      ],
      "processors": [
        "grok",
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "now()"
    ],
    "yaml": "input:\n  file:\n    paths:\n      - /var/log/app.log\n    codec: lines\n\npipeline:\n  processors:\n    - grok:\n        expressions:\n          - '%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}'\n    - mapping: |\n        root = this\n        root.parsed_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "http-camera-capture",
    "name": "HTTP Camera Image Capture",
    "description": "Capture images from network camera and encode to base64",
    "keywords": [
      "http",
      "camera",
      "image",
      "capture",
      "base64",
      "thermal"
    ],
    "components": {
      "inputs": [
        "http_client"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "content()",
      "encode(\"base64\")",
      "rate_limit_resources"
    ],
    "yaml": "input:\n  http_client:\n    url: http://camera-ip-address/snapshot.jpg\n    verb: GET\n    rate_limit: webcam_frequency\n    timeout: 5s\n    retries: 3\n\npipeline:\n  processors:\n    - mapping: |\n        let image_base64 = content().encode(\"base64\").string()\n        let timestamp = (timestamp_unix_nano() / 1000000).floor()\n        root = {\n          \"timestamp_ms\": $timestamp,\n          \"image_base64\": $image_base64\n        }\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: camera-images\n\nrate_limit_resources:\n  - label: webcam_frequency\n    local:\n      count: 1\n      interval: 1s"
  },
  {
    "id": "http-client-enrichment",
    "name": "HTTP API Enrichment",
    "description": "Enrich messages by calling an external HTTP API",
    "keywords": [
      "http",
      "api",
      "enrichment",
      "lookup",
      "external",
      "rest"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "http",
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()",
      "env()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - raw-data\n\npipeline:\n  processors:\n    - http:\n        url: https://api.example.com/lookup/${! this.id }\n        verb: GET\n        headers:\n          Authorization: Bearer ${! env(\"API_TOKEN\") }\n    - mapping: |\n        root = this.parse_json()\n        root.enriched_at = now()\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: enriched-data"
  },
  {
    "id": "http-client-stdout",
    "name": "HTTP Client to Stdout",
    "description": "Fetch data from HTTP endpoint and print to stdout",
    "keywords": [
      "http",
      "client",
      "stdout",
      "api",
      "fetch",
      "get",
      "request"
    ],
    "components": {
      "inputs": [
        "http_client"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()"
    ],
    "yaml": "input:\n  http_client:\n    url: https://api.example.com/data\n    verb: GET\n    rate_limit: 1s\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "http-request-processor",
    "name": "HTTP Request Processor",
    "description": "Process HTTP requests from a server input",
    "keywords": [
      "http",
      "request",
      "processor",
      "server",
      "process",
      "handle"
    ],
    "components": {
      "inputs": [
        "http_server"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  http_server:\n    address: 0.0.0.0:8080\n    path: /process\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.processed_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "http-response-transformer",
    "name": "HTTP Response Transformer",
    "description": "Transform HTTP responses from an API",
    "keywords": [
      "http",
      "response",
      "transformer",
      "transform",
      "api",
      "client"
    ],
    "components": {
      "inputs": [
        "http_client"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  http_client:\n    url: https://api.example.com/data\n    verb: GET\n    rate_limit: 1s\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.transformed = true\n        root.fetched_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "http-to-kafka",
    "name": "HTTP to Kafka Pipeline",
    "description": "Receive HTTP requests and forward to Kafka",
    "keywords": [
      "http",
      "kafka",
      "webhook",
      "ingest",
      "forward",
      "publish"
    ],
    "components": {
      "inputs": [
        "http_server"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  http_server:\n    address: 0.0.0.0:8080\n    path: /ingest\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.ingested_at = now()\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: ingested-events"
  },
  {
    "id": "json-to-csv",
    "name": "JSON to CSV Conversion",
    "description": "Convert JSON messages to CSV format",
    "keywords": [
      "json",
      "csv",
      "convert",
      "transform",
      "format"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "file"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "let",
      "join()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - json-data\n\npipeline:\n  processors:\n    - mapping: |\n        let data = this.parse_json()\n        root = [$data.id, $data.name, $data.email, $data.created_at].join(\",\")\n\noutput:\n  file:\n    path: /data/output.csv\n    codec: lines"
  },
  {
    "id": "json-to-xml",
    "name": "JSON to XML Conversion",
    "description": "Convert JSON objects to XML format",
    "keywords": [
      "json",
      "xml",
      "convert",
      "format",
      "transform"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "format_xml()"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics: [json-data]\n\npipeline:\n  processors:\n    - mapping: |\n        root.xml_data = this.json_data.format_xml()\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: xml-data"
  },
  {
    "id": "kafka-batching",
    "name": "Kafka Batching Example",
    "description": "Consume Kafka messages with batching configuration",
    "keywords": [
      "kafka",
      "batch",
      "batching",
      "batched",
      "batch size",
      "buffer"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "uuid_v4()",
      "map_each()",
      "parse_json()",
      "length()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n    consumer_group: batch-consumer\n    batching:\n      count: 100\n      period: 10s\n\npipeline:\n  processors:\n    - mapping: |\n        root.batch_id = uuid_v4()\n        root.messages = this.map_each(msg -> msg.parse_json())\n        root.count = this.length()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "kafka-cache-aggregation",
    "name": "Kafka Cache Aggregation",
    "description": "Aggregate messages from multiple topics using cache",
    "keywords": [
      "cache",
      "aggregate",
      "kafka",
      "combine",
      "multiple",
      "topics"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "branch",
        "cache",
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "cache processor",
      "match expression"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics:\n      - classification.topic1\n      - classification.topic2\n    consumer_group: aggregator\n\npipeline:\n  processors:\n    - branch:\n        processors:\n          - cache:\n              resource: memorycache\n              operator: set\n              key: ${! meta(\"kafka_topic\") }\n              value: ${! json(\"classification\") }\n    - branch:\n        processors:\n          - cache:\n              resource: memorycache\n              operator: get\n              key: classification.topic1\n        result_map: root.class_1 = content().string()\n    - branch:\n        processors:\n          - cache:\n              resource: memorycache\n              operator: get\n              key: classification.topic2\n        result_map: root.class_2 = content().string()\n    - mapping: |\n        root.final = match {\n          this.class_1 == \"off\" || this.class_2 == \"off\" => \"Machine-off\",\n          this.class_1 == \"on\" && this.class_2 == \"good\" => \"good\",\n          _ => \"Unknown\"\n        }\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: aggregated-result\n\ncache_resources:\n  - label: memorycache\n    memory:\n      default_ttl: 5m"
  },
  {
    "id": "kafka-consumer",
    "name": "Kafka Consumer Example",
    "description": "Consume messages from a Kafka topic",
    "keywords": [
      "kafka",
      "consumer",
      "consume",
      "read",
      "subscribe",
      "topic"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - my-topic\n    consumer_group: my-consumer-group\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "kafka-filter-events",
    "name": "Kafka Event Filtering",
    "description": "Filter Kafka events by type and forward matching ones",
    "keywords": [
      "kafka",
      "filter",
      "events",
      "conditional",
      "routing"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "if { } else { }",
      "deleted()",
      "let"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - raw-events\n    consumer_group: event-filter\n\npipeline:\n  processors:\n    - mapping: |\n        let event = this.parse_json()\n        root = if $event.type == \"purchase\" {\n          $event\n        } else {\n          deleted()\n        }\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: purchase-events"
  },
  {
    "id": "kafka-influxdb-lineprotocol",
    "name": "Kafka to InfluxDB Line Protocol",
    "description": "Transform Kafka messages to InfluxDB line protocol format using topic metadata",
    "keywords": [
      "kafka",
      "influxdb",
      "timeseries",
      "metrics",
      "line protocol",
      "http"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "http_client"
      ]
    },
    "bloblangPatterns": [
      "content()",
      "number()",
      "string()",
      "split()",
      "index()",
      "meta()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - 'ia.raw.development.ioTSensors.*'\n    consumer_group: influx-writer\n\npipeline:\n  processors:\n    - mapping: |\n        let value = content().number().string()\n        # Split topic: ia.raw.<measurement>.<location>.<field>\n        root = meta(\"kafka_topic\").string().split(\".\").index(2) +\n               \",sensor=\" + meta(\"kafka_topic\").string().split(\".\").index(3) + \" \" +\n               meta(\"kafka_topic\").string().split(\".\").index(4) + \"=\" + $value + \" \"\n\noutput:\n  http_client:\n    url: 'http://influxdb:8086/api/v2/write?org=myorg&bucket=mybucket'\n    verb: POST\n    headers:\n      Authorization: 'Token ${INFLUX_TOKEN}'\n      Content-Type: text/plain"
  },
  {
    "id": "kafka-mongodb-insert",
    "name": "Kafka to MongoDB",
    "description": "Insert Kafka messages into MongoDB with automatic timestamps",
    "keywords": [
      "kafka",
      "mongodb",
      "database",
      "insert",
      "nosql",
      "document"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "mongodb"
      ]
    },
    "bloblangPatterns": [
      "timestamp_unix()",
      "document_map",
      "object literal"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - my-events\n    consumer_group: mongo-writer\n\npipeline:\n  processors:\n    - mapping: |\n        root = {\n          \"message\": this,\n          \"timestamp_unix\": timestamp_unix()\n        }\n\noutput:\n  mongodb:\n    url: mongodb://localhost:27017\n    database: mydb\n    collection: events\n    operation: insert-one\n    document_map: |\n      root.message = this.message\n      root.timestamp_unix = this.timestamp_unix"
  },
  {
    "id": "kafka-offset",
    "name": "Kafka Offset Management",
    "description": "Manage Kafka consumer offsets",
    "keywords": [
      "kafka",
      "offset",
      "management",
      "consumer",
      "commit",
      "position"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "meta()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - my-topic\n    consumer_group: offset-manager\n    start_from_oldest: true\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.offset = meta(\"kafka_offset\")\n        root.partition = meta(\"kafka_partition\")\n\noutput:\n  stdout: {}"
  },
  {
    "id": "kafka-producer",
    "name": "Kafka Producer Pipeline",
    "description": "Produce messages to a Kafka topic",
    "keywords": [
      "kafka",
      "producer",
      "produce",
      "write",
      "publish",
      "topic"
    ],
    "components": {
      "inputs": [
        "generate"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "uuid_v4()",
      "now()"
    ],
    "yaml": "input:\n  generate:\n    count: 100\n    interval: 100ms\n    mapping: |\n      root.id = uuid_v4()\n      root.timestamp = now()\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: my-output-topic"
  },
  {
    "id": "kafka-to-elasticsearch",
    "name": "Kafka to Elasticsearch",
    "description": "Stream Kafka messages to Elasticsearch for indexing",
    "keywords": [
      "kafka",
      "elasticsearch",
      "elastic",
      "index",
      "search",
      "streaming"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "elasticsearch"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()",
      "format_timestamp()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n    consumer_group: es-indexer\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.indexed_at = now()\n\noutput:\n  elasticsearch:\n    urls:\n      - http://localhost:9200\n    index: events-${! now().format_timestamp(\"2006.01.02\") }"
  },
  {
    "id": "kafka-to-kafka",
    "name": "Kafka to Kafka Transform",
    "description": "Read from one Kafka topic, transform, and write to another",
    "keywords": [
      "kafka",
      "transform",
      "topic",
      "streaming",
      "etl"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - raw-events\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.processed = true\n        root.timestamp = now()\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: processed-events"
  },
  {
    "id": "kafka-to-redis",
    "name": "Kafka to Redis Pipeline",
    "description": "Stream Kafka messages to Redis for caching",
    "keywords": [
      "kafka",
      "redis",
      "cache",
      "streaming",
      "key-value"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "redis_hash"
      ]
    },
    "bloblangPatterns": [
      "parse_json()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - users\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n\noutput:\n  redis_hash:\n    url: redis://localhost:6379\n    key: user:${! this.id }\n    fields_mapping: |\n      root = this"
  },
  {
    "id": "kafka-to-s3-json",
    "name": "Kafka to S3 with JSON Parsing",
    "description": "Consume Kafka messages, parse JSON, and write to S3",
    "keywords": [
      "kafka",
      "s3",
      "json",
      "parse",
      "aws",
      "streaming",
      "etl"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "aws_s3"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()",
      "timestamp_unix()",
      "uuid_v4()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - my-topic\n    consumer_group: my-group\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.data = this.value.parse_json()\n        root.timestamp = now()\n\noutput:\n  aws_s3:\n    bucket: my-bucket\n    path: ${! timestamp_unix() }/${! uuid_v4() }.json"
  },
  {
    "id": "kinesis-to-stdout",
    "name": "Kinesis Stream Processor",
    "description": "Read from AWS Kinesis stream and process messages",
    "keywords": [
      "kinesis",
      "aws",
      "stream",
      "processor",
      "streaming",
      "real-time"
    ],
    "components": {
      "inputs": [
        "aws_kinesis"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  aws_kinesis:\n    stream: my-stream\n    dynamodb_table: kinesis_checkpoints\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.processed_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "lambda-trigger",
    "name": "Lambda Trigger Pipeline",
    "description": "Pipeline triggered by AWS Lambda events",
    "keywords": [
      "lambda",
      "trigger",
      "aws",
      "serverless",
      "event",
      "function"
    ],
    "components": {
      "inputs": [
        "aws_sqs"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  aws_sqs:\n    url: https://sqs.us-east-1.amazonaws.com/123456789/lambda-trigger-queue\n    region: us-east-1\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.triggered_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "latency-measurement",
    "name": "Message Latency Measurement",
    "description": "Measure processing latency between timestamps",
    "keywords": [
      "latency",
      "timing",
      "performance",
      "measurement",
      "timestamp"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "timestamp_unix_nano()",
      "exists()",
      "dynamic field names"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics: [events]\n\npipeline:\n  processors:\n    - mapping: |\n        root = if !this.exists(\"timestamp_ms\") { deleted() }\n    - mapping: |\n        let current_timestamp = (timestamp_unix_nano() / 1000000).floor()\n        let original_timestamp = this.timestamp_ms.number()\n        let latency = $current_timestamp - $original_timestamp\n        let process_value_name = meta(\"kafka_topic\").replace_all(\".\", \"_\")\n        root = {\n          \"timestamp_ms\": $current_timestamp,\n          $process_value_name: $latency\n        }\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: latency-metrics"
  },
  {
    "id": "llm-classification",
    "name": "LLM Text Classification",
    "description": "Classify text using LLM with structured output",
    "keywords": [
      "llm",
      "classify",
      "classification",
      "ai",
      "ml",
      "openai",
      "category",
      "label",
      "nlp"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "openai_chat_completion",
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "env()",
      "index()",
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - support-tickets\n\npipeline:\n  processors:\n    - mapping: |\n        root.messages = [\n          {\n            \"role\": \"system\",\n            \"content\": \"Classify the support ticket. Respond with JSON: {\"category\": \"billing|technical|general\", \"priority\": \"low|medium|high\", \"sentiment\": \"positive|neutral|negative\"}\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": this.content\n          }\n        ]\n    - openai_chat_completion:\n        api_key: ${! env(\"OPENAI_API_KEY\") }\n        model: gpt-4o-mini\n        response_format:\n          type: json_object\n    - mapping: |\n        root.ticket_id = this.ticket_id\n        root.content = this.content\n        root.classification = this.choices.index(0).message.content.parse_json()\n        root.classified_at = now()\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: classified-tickets"
  },
  {
    "id": "log-aggregation-simple",
    "name": "Simple Log Aggregation",
    "description": "Aggregate logs from files and write to Elasticsearch",
    "keywords": [
      "log",
      "aggregation",
      "aggregate",
      "logs",
      "collect",
      "elasticsearch",
      "file",
      "parsing"
    ],
    "components": {
      "inputs": [
        "file"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "elasticsearch_v8"
      ]
    },
    "bloblangPatterns": [
      "now()",
      "env()",
      "meta()",
      "format_timestamp()"
    ],
    "yaml": "input:\n  file:\n    paths:\n      - /var/log/*.log\n    codec: lines\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.timestamp = now()\n        root.hostname = env(\"HOSTNAME\")\n        root.source = meta(\"path\")\n\noutput:\n  elasticsearch_v8:\n    urls:\n      - http://localhost:9200\n    index: logs-${! now().format_timestamp(\"2006.01.02\") }"
  },
  {
    "id": "log-parsing",
    "name": "Log Parsing and Enrichment",
    "description": "Parse logs with grok patterns and enrich with metadata",
    "keywords": [
      "log",
      "logs",
      "parse",
      "grok",
      "observability",
      "monitoring"
    ],
    "components": {
      "inputs": [
        "file"
      ],
      "processors": [
        "grok",
        "mapping"
      ],
      "outputs": [
        "elasticsearch_v8"
      ]
    },
    "bloblangPatterns": [
      "parse_timestamp_strptime()",
      "env()"
    ],
    "yaml": "input:\n  file:\n    paths:\n      - /var/log/app/*.log\n    codec: lines\n\npipeline:\n  processors:\n    - grok:\n        expressions:\n          - '%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}'\n    - mapping: |\n        root = this\n        root.@timestamp = this.timestamp.parse_timestamp_strptime(\"%Y-%m-%dT%H:%M:%S\")\n        root.hostname = env(\"HOSTNAME\")\n\noutput:\n  elasticsearch_v8:\n    urls:\n      - http://localhost:9200\n    index: app-logs-%{+2006.01.02}"
  },
  {
    "id": "match-routing",
    "name": "Match Expression Routing",
    "description": "Route messages using match expressions (Modbus example)",
    "keywords": [
      "match",
      "route",
      "routing",
      "conditional",
      "modbus",
      "switch"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "match expression",
      "meta()",
      "let variable"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics: [modbus-data]\n\npipeline:\n  processors:\n    - mapping: |\n        let workcell = match {\n          meta(\"modbus_slave_id\") == \"10\" => \"Rack_ID_10\",\n          meta(\"modbus_slave_id\") == \"20\" => \"Rack_ID_20\",\n          meta(\"modbus_slave_id\") == \"30\" => \"Rack_ID_30\",\n          _ => \"Unknown_Rack\"\n        }\n        root = this\n        root.workcell = $workcell\n        root.tagname = meta(\"modbus_tag_name\")\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: routed-data"
  },
  {
    "id": "metrics-aggregation-simple",
    "name": "Simple Metrics Aggregation",
    "description": "Aggregate metrics from Kafka and write summaries",
    "keywords": [
      "metrics",
      "aggregation",
      "aggregate",
      "prometheus",
      "kafka",
      "batch",
      "window"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "aws_s3"
      ]
    },
    "bloblangPatterns": [
      "uuid_v4()",
      "map_each()",
      "parse_json()",
      "length()",
      "now()",
      "format_timestamp()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - metrics\n    batching:\n      count: 100\n      period: 10s\n\npipeline:\n  processors:\n    - mapping: |\n        root.window_id = uuid_v4()\n        root.metrics = this.map_each(m -> m.parse_json())\n        root.count = this.length()\n        root.window_end = now()\n\noutput:\n  aws_s3:\n    bucket: metrics-archive\n    path: ${! now().format_timestamp(\"2006/01/02\") }/${! this.window_id }.json"
  },
  {
    "id": "mongodb-cdc-to-kafka",
    "name": "MongoDB CDC to Kafka",
    "description": "Stream MongoDB changes to Kafka",
    "keywords": [
      "mongodb",
      "cdc",
      "change",
      "stream",
      "kafka",
      "realtime"
    ],
    "components": {
      "inputs": [
        "mongodb_cdc"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "string()"
    ],
    "yaml": "input:\n  mongodb_cdc:\n    url: mongodb://localhost:27017\n    database: mydb\n    collection: orders\n\npipeline:\n  processors:\n    - mapping: |\n        root.operation = this.operationType\n        root.document = this.fullDocument\n        root.timestamp = this.clusterTime.string()\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: order-changes"
  },
  {
    "id": "mqtt-bridge",
    "name": "MQTT Broker Bridge",
    "description": "Bridge external MQTT broker to internal system",
    "keywords": [
      "mqtt",
      "bridge",
      "import",
      "broker",
      "external"
    ],
    "components": {
      "inputs": [
        "mqtt"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "meta(\"mqtt_topic\")",
      "catch()",
      "null check"
    ],
    "yaml": "input:\n  mqtt:\n    urls:\n      - tcp://external-broker:1883\n    topics:\n      - external/#\n    client_id: mqtt-bridge\n\npipeline:\n  processors:\n    - mapping: |\n        let mqtt_topic = meta(\"mqtt_topic\").replace_all(\"/\", \".\")\n        let payload = this.catch(deleted())\n        root = if payload != null { payload } else { deleted() }\n        root.source_topic = $mqtt_topic\n        root.imported_at = now()\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: imported-mqtt"
  },
  {
    "id": "mqtt-ssl-tls",
    "name": "MQTT with SSL/TLS",
    "description": "Connect to MQTT broker with SSL/TLS and self-signed certificates",
    "keywords": [
      "mqtt",
      "ssl",
      "tls",
      "certificate",
      "secure",
      "encrypted"
    ],
    "components": {
      "inputs": [
        "mqtt"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "tls configuration",
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  mqtt:\n    urls:\n      - ssl://secure-broker:8883\n    topics:\n      - secure-topic/#\n    client_id: ssl-client\n    tls:\n      enabled: true\n      skip_cert_verify: true\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.received_at = now()\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: secure-data"
  },
  {
    "id": "multi-output-broker",
    "name": "Multi-Output Broker",
    "description": "Write to multiple outputs simultaneously",
    "keywords": [
      "broker",
      "multi",
      "output",
      "duplicate",
      "copy",
      "multiple",
      "destinations",
      "fanout"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "broker"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()",
      "format_timestamp()",
      "uuid_v4()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.replicated_at = now()\n\noutput:\n  broker:\n    pattern: fan_out\n    outputs:\n      - kafka:\n          addresses:\n            - localhost:9092\n          topic: events-primary\n      - aws_s3:\n          bucket: events-archive\n          path: ${! now().format_timestamp(\"2006/01/02\") }/${! uuid_v4() }.json\n      - elasticsearch_v8:\n          urls:\n            - http://localhost:9200\n          index: events"
  },
  {
    "id": "multi-topic-kafka",
    "name": "Multi-Topic Kafka Consumer",
    "description": "Consume messages from multiple Kafka topics",
    "keywords": [
      "kafka",
      "multi",
      "topic",
      "multiple",
      "topics",
      "consumer"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "meta()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - topic-one\n      - topic-two\n      - topic-three\n    consumer_group: multi-topic-consumer\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.source_topic = meta(\"kafka_topic\")\n        root.processed_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "mysql-cdc-to-elasticsearch",
    "name": "MySQL CDC to Elasticsearch",
    "description": "Stream MySQL changes to Elasticsearch for search indexing",
    "keywords": [
      "mysql",
      "cdc",
      "elasticsearch",
      "search",
      "index",
      "binlog",
      "replication",
      "sync"
    ],
    "components": {
      "inputs": [
        "mysql_cdc"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "elasticsearch_v8"
      ]
    },
    "bloblangPatterns": [
      "match",
      "string()",
      "now()",
      "or()"
    ],
    "yaml": "input:\n  mysql_cdc:\n    dsn: user:pass@tcp(localhost:3306)/mydb\n    tables:\n      - products\n    snapshot_mode: initial\n\npipeline:\n  processors:\n    - mapping: |\n        root = match this.operation {\n          \"delete\" => {\n            \"_op_type\": \"delete\",\n            \"_id\": this.before.id.string()\n          }\n          _ => {\n            \"_id\": this.after.id.string(),\n            \"name\": this.after.name,\n            \"description\": this.after.description,\n            \"price\": this.after.price,\n            \"updated_at\": now()\n          }\n        }\n\noutput:\n  elasticsearch_v8:\n    urls:\n      - http://localhost:9200\n    index: products\n    id: ${! this._id }\n    action: ${! this._op_type.or(\"index\") }"
  },
  {
    "id": "nats-request-reply",
    "name": "NATS Request-Reply",
    "description": "Implement request-reply pattern with NATS",
    "keywords": [
      "nats",
      "request",
      "reply",
      "rpc",
      "sync",
      "pattern",
      "messaging",
      "queue"
    ],
    "components": {
      "inputs": [
        "nats"
      ],
      "processors": [
        "mapping",
        "nats_request_reply"
      ],
      "outputs": [
        "nats"
      ]
    },
    "bloblangPatterns": [
      "uuid_v4()",
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  nats:\n    urls:\n      - nats://localhost:4222\n    subject: requests\n\npipeline:\n  processors:\n    - mapping: |\n        root.request_id = uuid_v4()\n        root.payload = this.parse_json()\n    - nats_request_reply:\n        urls:\n          - nats://localhost:4222\n        subject: backend.process\n        timeout: 5s\n    - mapping: |\n        root.request_id = this.request_id\n        root.response = this.parse_json()\n        root.completed_at = now()\n\noutput:\n  nats:\n    urls:\n      - nats://localhost:4222\n    subject: responses"
  },
  {
    "id": "numeric-calculations",
    "name": "Numeric Calculations",
    "description": "Perform mathematical operations on message values",
    "keywords": [
      "math",
      "calculate",
      "multiply",
      "divide",
      "number",
      "arithmetic"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "number()",
      "arithmetic operators"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics: [sensor-data]\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.result = this.value.number() * 10\n        root.celsius = (this.fahrenheit.number() - 32) * 5 / 9\n        root.percentage = (this.current / this.max) * 100\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: calculated-data"
  },
  {
    "id": "ollama-chat",
    "name": "Local LLM Chat with Ollama",
    "description": "Process messages through local Ollama LLM",
    "keywords": [
      "ollama",
      "llm",
      "chat",
      "ai",
      "local",
      "inference"
    ],
    "components": {
      "inputs": [
        "http_server"
      ],
      "processors": [
        "ollama_chat",
        "mapping"
      ],
      "outputs": [
        "sync_response"
      ]
    },
    "bloblangPatterns": [],
    "yaml": "input:\n  http_server:\n    address: 0.0.0.0:8080\n    path: /chat\n    allowed_verbs:\n      - POST\n\npipeline:\n  processors:\n    - mapping: |\n        root.prompt = this.message\n    - ollama_chat:\n        server_address: http://localhost:11434\n        model: llama3.2\n    - mapping: |\n        root.response = this.response\n        root.model = \"llama3.2\"\n\noutput:\n  sync_response: {}"
  },
  {
    "id": "opcua-mqtt-bridge",
    "name": "OPC-UA to MQTT Bridge",
    "description": "Bridge OPC-UA data to MQTT with dynamic topic paths from node metadata",
    "keywords": [
      "opcua",
      "mqtt",
      "iiot",
      "plc",
      "industrial",
      "bridge",
      "ot"
    ],
    "components": {
      "inputs": [
        "opcua"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "mqtt"
      ]
    },
    "bloblangPatterns": [
      "meta()",
      "timestamp_unix()",
      "dynamic topic interpolation"
    ],
    "yaml": "input:\n  opcua:\n    endpoint: 'opc.tcp://localhost:46010'\n    nodeIDs: ['ns=2;s=IoTSensors']\n\npipeline:\n  processors:\n    - mapping: |\n        root = {\n          meta(\"opcua_path\"): this,\n          \"timestamp_unix\": timestamp_unix()\n        }\n\noutput:\n  mqtt:\n    urls:\n      - 'localhost:1883'\n    topic: 'ia/raw/opcuasimulator/${! meta(\"opcua_path\") }'\n    client_id: 'benthos-umh'"
  },
  {
    "id": "openai-embedding",
    "name": "OpenAI Embedding Generation",
    "description": "Generate embeddings for text using OpenAI API",
    "keywords": [
      "openai",
      "embedding",
      "ai",
      "ml",
      "vector",
      "rag"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "openai_embeddings",
        "mapping"
      ],
      "outputs": [
        "qdrant"
      ]
    },
    "bloblangPatterns": [
      "env()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - documents\n\npipeline:\n  processors:\n    - mapping: |\n        root.id = this.id\n        root.text = this.content\n    - openai_embeddings:\n        api_key: ${! env(\"OPENAI_API_KEY\") }\n        model: text-embedding-3-small\n    - mapping: |\n        root.id = this.id\n        root.vector = this.embedding\n        root.payload = {\"text\": this.text}\n\noutput:\n  qdrant:\n    grpc_host: localhost:6334\n    collection: documents"
  },
  {
    "id": "parallel-enrichment",
    "name": "Parallel HTTP Enrichment",
    "description": "Enrich messages from multiple APIs in parallel",
    "keywords": [
      "parallel",
      "enrich",
      "http",
      "api",
      "concurrent",
      "lookup",
      "multiple",
      "sources"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "parallel",
        "http",
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - users\n\npipeline:\n  processors:\n    - parallel:\n        cap: 10\n        processors:\n          - branch:\n              request_map: 'root = this.user_id'\n              processors:\n                - http:\n                    url: https://api.example.com/user/${! this }\n                    verb: GET\n              result_map: 'root.profile = this.parse_json()'\n          - branch:\n              request_map: 'root = this.user_id'\n              processors:\n                - http:\n                    url: https://api.example.com/preferences/${! this }\n                    verb: GET\n              result_map: 'root.preferences = this.parse_json()'\n    - mapping: |\n        root = this\n        root.enriched_at = now()\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: enriched-users"
  },
  {
    "id": "passthrough",
    "name": "Passthrough Pipeline",
    "description": "Simple passthrough that echoes input to output",
    "keywords": [
      "passthrough",
      "pass",
      "through",
      "echo",
      "simple",
      "basic",
      "identity",
      "noop"
    ],
    "components": {
      "inputs": [
        "stdin"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [],
    "yaml": "input:\n  stdin: {}\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n\noutput:\n  stdout: {}"
  },
  {
    "id": "pii-redaction",
    "name": "PII Redaction",
    "description": "Redact personally identifiable information from messages",
    "keywords": [
      "pii",
      "redact",
      "privacy",
      "security",
      "mask",
      "gdpr",
      "compliance"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "re_replace_all()",
      "if { } else { }",
      "deleted()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - raw-data\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.email = this.email.re_replace_all(\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", \"[REDACTED]\")\n        root.phone = this.phone.re_replace_all(\"\\d{3}-\\d{3}-\\d{4}\", \"[REDACTED]\")\n        root.ssn = if this.ssn != null { \"[REDACTED]\" } else { deleted() }\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: sanitized-data"
  },
  {
    "id": "postgres-cdc-to-kafka",
    "name": "PostgreSQL CDC to Kafka",
    "description": "Stream PostgreSQL changes to Kafka using CDC",
    "keywords": [
      "postgres",
      "postgresql",
      "cdc",
      "change",
      "data",
      "capture",
      "kafka",
      "replication",
      "wal"
    ],
    "components": {
      "inputs": [
        "postgres_cdc"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "now()"
    ],
    "yaml": "input:\n  postgres_cdc:\n    dsn: postgres://user:pass@localhost:5432/mydb?replication=database\n    slot_name: my_slot\n    tables:\n      - public.orders\n      - public.customers\n\npipeline:\n  processors:\n    - mapping: |\n        root.table = this.table\n        root.operation = this.operation\n        root.data = this.after\n        root.before = this.before\n        root.timestamp = now()\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: cdc-${! this.table }"
  },
  {
    "id": "protobuf-decoder",
    "name": "Protobuf to JSON",
    "description": "Decode Protobuf messages to JSON",
    "keywords": [
      "protobuf",
      "proto",
      "json",
      "decode",
      "convert",
      "binary"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "protobuf",
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - protobuf-events\n\npipeline:\n  processors:\n    - protobuf:\n        operator: to_json\n        message: my.package.MyMessage\n        import_paths:\n          - /proto\n    - mapping: |\n        root = this\n        root.decoded_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "pubsub-filtering",
    "name": "GCP Pub/Sub with Filtering",
    "description": "Filter and route GCP Pub/Sub messages by attributes",
    "keywords": [
      "pubsub",
      "gcp",
      "google",
      "filter",
      "attributes",
      "route",
      "cloud",
      "messaging"
    ],
    "components": {
      "inputs": [
        "gcp_pubsub"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "switch"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "meta()",
      "or()"
    ],
    "yaml": "input:\n  gcp_pubsub:\n    project: my-project\n    subscription: my-subscription\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.event_type = meta(\"event_type\")\n        root.priority = meta(\"priority\").or(\"normal\")\n\noutput:\n  switch:\n    cases:\n      - check: this.priority == \"high\"\n        output:\n          gcp_pubsub:\n            project: my-project\n            topic: high-priority\n      - check: this.event_type == \"error\"\n        output:\n          gcp_pubsub:\n            project: my-project\n            topic: errors\n      - output:\n          gcp_pubsub:\n            project: my-project\n            topic: standard"
  },
  {
    "id": "rabbitmq-fanout",
    "name": "RabbitMQ Fanout Pattern",
    "description": "Consume from RabbitMQ and fan-out to multiple queues",
    "keywords": [
      "rabbitmq",
      "amqp",
      "fanout",
      "exchange",
      "broadcast",
      "pubsub"
    ],
    "components": {
      "inputs": [
        "amqp_0_9"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "broker",
        "amqp_0_9"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  amqp_0_9:\n    urls:\n      - amqp://guest:guest@localhost:5672/\n    queue: source-queue\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.distributed_at = now()\n\noutput:\n  broker:\n    pattern: fan_out\n    outputs:\n      - amqp_0_9:\n          urls:\n            - amqp://guest:guest@localhost:5672/\n          exchange: fanout-exchange\n          key: queue-1\n      - amqp_0_9:\n          urls:\n            - amqp://guest:guest@localhost:5672/\n          exchange: fanout-exchange\n          key: queue-2"
  },
  {
    "id": "rag-embedding-pipeline",
    "name": "RAG Embedding Pipeline",
    "description": "Generate embeddings for RAG applications with chunking",
    "keywords": [
      "rag",
      "embedding",
      "vector",
      "chunk",
      "ai",
      "ml",
      "retrieval",
      "augmented",
      "generation"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "text_chunker",
        "openai_embeddings",
        "mapping"
      ],
      "outputs": [
        "qdrant"
      ]
    },
    "bloblangPatterns": [
      "env()",
      "uuid_v4()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - documents\n\npipeline:\n  processors:\n    - mapping: |\n        root.doc_id = this.id\n        root.text = this.content\n    - text_chunker:\n        chunk_size: 512\n        chunk_overlap: 50\n    - openai_embeddings:\n        api_key: ${! env(\"OPENAI_API_KEY\") }\n        model: text-embedding-3-small\n    - mapping: |\n        root.id = uuid_v4()\n        root.doc_id = this.doc_id\n        root.chunk_index = this.chunk_index\n        root.vector = this.embedding\n        root.payload = {\n          \"text\": this.text,\n          \"doc_id\": this.doc_id,\n          \"chunk_index\": this.chunk_index\n        }\n\noutput:\n  qdrant:\n    grpc_host: localhost:6334\n    collection: document_chunks"
  },
  {
    "id": "random-number-generation",
    "name": "Random Number Generation",
    "description": "Generate random numbers for testing or validation",
    "keywords": [
      "random",
      "number",
      "generate",
      "test",
      "validation"
    ],
    "components": {
      "inputs": [
        "generate"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "random_int()",
      "timestamp_unix_nano()",
      "floor()"
    ],
    "yaml": "input:\n  generate:\n    count: 100\n    interval: 100ms\n    mapping: |\n      root = {}\n\npipeline:\n  processors:\n    - mapping: |\n        root.random_number = random_int(max: 100)\n        root.random_float = random_int(max: 10000) / 100\n        root.timestamp_ms = (timestamp_unix_nano() / 1000000).floor()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "rate-limited-processing",
    "name": "Rate Limited Processing",
    "description": "Process messages with rate limiting to protect downstream services",
    "keywords": [
      "rate",
      "limit",
      "throttle",
      "backpressure",
      "protect",
      "downstream",
      "control",
      "flow"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "rate_limit",
        "http",
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - requests\n\npipeline:\n  processors:\n    - rate_limit:\n        resource: api_limiter\n        count: 100\n    - http:\n        url: https://api.example.com/process\n        verb: POST\n    - mapping: |\n        root = this.parse_json()\n        root.rate_limited_at = now()\n\nrate_limit_resources:\n  - label: api_limiter\n    local:\n      count: 100\n      interval: 1s\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: processed"
  },
  {
    "id": "redis-streams-consumer",
    "name": "Redis Streams Consumer Group",
    "description": "Process Redis Streams with consumer groups",
    "keywords": [
      "redis",
      "streams",
      "consumer",
      "group",
      "queue",
      "distributed",
      "messaging"
    ],
    "components": {
      "inputs": [
        "redis_streams"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "redis_streams"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "env()",
      "now()"
    ],
    "yaml": "input:\n  redis_streams:\n    url: redis://localhost:6379\n    body_key: body\n    streams:\n      - stream: orders:new\n        consumer_group: order-processor\n        consumer_name: worker-1\n        start_from_oldest: true\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.processed_by = env(\"HOSTNAME\")\n        root.processed_at = now()\n\noutput:\n  redis_streams:\n    url: redis://localhost:6379\n    stream: orders:processed\n    max_length: 10000"
  },
  {
    "id": "rest-api-polling",
    "name": "REST API Polling Pipeline",
    "description": "Poll a REST API endpoint periodically and process responses",
    "keywords": [
      "rest",
      "api",
      "polling",
      "poll",
      "http",
      "periodic",
      "interval"
    ],
    "components": {
      "inputs": [
        "generate",
        "http"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  generate:\n    interval: 30s\n    mapping: |\n      root = {}\n\npipeline:\n  processors:\n    - http:\n        url: https://api.example.com/status\n        verb: GET\n    - mapping: |\n        root = this.parse_json()\n        root.polled_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "retry-with-backoff",
    "name": "Retry with Exponential Backoff",
    "description": "Retry failed operations with exponential backoff",
    "keywords": [
      "retry",
      "backoff",
      "exponential",
      "error",
      "resilience",
      "fault",
      "tolerance",
      "recover",
      "failure"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "retry",
        "http",
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - orders\n\npipeline:\n  processors:\n    - retry:\n        max_retries: 5\n        backoff:\n          initial_interval: 1s\n          max_interval: 30s\n          max_elapsed_time: 5m\n        processors:\n          - http:\n              url: https://api.example.com/process\n              verb: POST\n              headers:\n                Content-Type: application/json\n    - mapping: |\n        root = this.parse_json()\n        root.processed_at = now()\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: processed-orders"
  },
  {
    "id": "s3-file-processor",
    "name": "S3 File Processor",
    "description": "Process files from S3 bucket",
    "keywords": [
      "s3",
      "file",
      "processor",
      "process",
      "aws",
      "bucket"
    ],
    "components": {
      "inputs": [
        "aws_s3"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "meta()",
      "now()"
    ],
    "yaml": "input:\n  aws_s3:\n    bucket: my-bucket\n    prefix: incoming/\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.file_key = meta(\"s3_key\")\n        root.processed_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "s3-to-bigquery",
    "name": "S3 to BigQuery",
    "description": "Load S3 files into Google BigQuery",
    "keywords": [
      "s3",
      "bigquery",
      "gcp",
      "aws",
      "cloud",
      "data",
      "lake",
      "warehouse"
    ],
    "components": {
      "inputs": [
        "aws_s3"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "gcp_bigquery"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  aws_s3:\n    bucket: data-lake\n    prefix: raw/\n    scanner:\n      to: latest\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root._ingested_at = now()\n\noutput:\n  gcp_bigquery:\n    project: my-project\n    dataset: analytics\n    table: events"
  },
  {
    "id": "s3-to-stdout",
    "name": "S3 to Stdout",
    "description": "Read files from S3 bucket and print to stdout",
    "keywords": [
      "s3",
      "stdout",
      "aws",
      "read",
      "print",
      "console",
      "file",
      "bucket"
    ],
    "components": {
      "inputs": [
        "aws_s3"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "meta()"
    ],
    "yaml": "input:\n  aws_s3:\n    bucket: my-bucket\n    prefix: data/\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.file_path = meta(\"s3_key\")\n\noutput:\n  stdout: {}"
  },
  {
    "id": "shared-cache",
    "name": "Shared Cache Pattern",
    "description": "Use shared cache across processors for deduplication",
    "keywords": [
      "cache",
      "shared",
      "dedupe",
      "deduplication",
      "resource",
      "memory",
      "redis"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "cache",
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "errored()",
      "deleted()",
      "parse_json()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - cache:\n        resource: dedupe_cache\n        operator: add\n        key: ${! this.id }\n        value: \"1\"\n        ttl: 1h\n    - mapping: |\n        root = if errored() {\n          # Already seen this ID, skip\n          deleted()\n        } else {\n          this.parse_json()\n        }\n\ncache_resources:\n  - label: dedupe_cache\n    redis:\n      url: redis://localhost:6379\n      default_ttl: 1h\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: deduplicated-events"
  },
  {
    "id": "sns-to-sqs",
    "name": "SNS to SQS Pipeline",
    "description": "Fan out SNS messages to SQS queue",
    "keywords": [
      "sns",
      "sqs",
      "aws",
      "fanout",
      "queue",
      "pubsub"
    ],
    "components": {
      "inputs": [
        "aws_sqs"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "aws_sqs"
      ]
    },
    "bloblangPatterns": [
      "parse_json()"
    ],
    "yaml": "input:\n  aws_sqs:\n    url: https://sqs.us-east-1.amazonaws.com/123456789/source-queue\n    region: us-east-1\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.processed = true\n\noutput:\n  aws_sqs:\n    url: https://sqs.us-east-1.amazonaws.com/123456789/dest-queue\n    region: us-east-1"
  },
  {
    "id": "sql-enrichment",
    "name": "Database Enrichment (SQL)",
    "description": "Enrich messages by fetching data from SQL database",
    "keywords": [
      "sql",
      "database",
      "postgres",
      "mysql",
      "enrich",
      "lookup",
      "join"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "branch",
        "sql_select",
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "branch",
      "sql_select",
      "result_map"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics: [raw-events]\n\npipeline:\n  processors:\n    - branch:\n        processors:\n          - sql_select:\n              driver: postgres\n              dsn: postgres://user:pass@localhost:5432/mydb\n              table: asset_data\n              where: asset_id = ?\n              args_mapping: root = [this.asset_id]\n              columns:\n                - latest_value\n                - asset_name\n        result_map: |\n          root.enrichment = this\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: enriched-events"
  },
  {
    "id": "sql-to-elasticsearch",
    "name": "SQL to Elasticsearch",
    "description": "Query SQL database and index results in Elasticsearch",
    "keywords": [
      "sql",
      "postgres",
      "mysql",
      "elasticsearch",
      "database",
      "search",
      "index"
    ],
    "components": {
      "inputs": [
        "sql_select"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "elasticsearch_v8"
      ]
    },
    "bloblangPatterns": [
      "string()"
    ],
    "yaml": "input:\n  sql_select:\n    driver: postgres\n    dsn: postgres://user:pass@localhost:5432/mydb\n    table: users\n    columns:\n      - id\n      - name\n      - email\n      - created_at\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root._id = this.id.string()\n\noutput:\n  elasticsearch_v8:\n    urls:\n      - http://localhost:9200\n    index: users\n    id: ${! this._id }"
  },
  {
    "id": "sql-upsert-sync",
    "name": "SQL Upsert Synchronization",
    "description": "Upsert data from one database to another",
    "keywords": [
      "sql",
      "upsert",
      "sync",
      "database",
      "insert",
      "update",
      "replicate",
      "postgres",
      "mysql"
    ],
    "components": {
      "inputs": [
        "sql_select"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "sql_raw"
      ]
    },
    "bloblangPatterns": [
      "now()"
    ],
    "yaml": "input:\n  sql_select:\n    driver: postgres\n    dsn: postgres://user:pass@source:5432/db\n    table: products\n    columns: [\"*\"]\n    where: updated_at > NOW() - INTERVAL '1 hour'\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.synced_at = now()\n\noutput:\n  sql_raw:\n    driver: postgres\n    dsn: postgres://user:pass@target:5432/db\n    query: |\n      INSERT INTO products (id, name, price, synced_at)\n      VALUES ($1, $2, $3, $4)\n      ON CONFLICT (id) DO UPDATE SET\n        name = EXCLUDED.name,\n        price = EXCLUDED.price,\n        synced_at = EXCLUDED.synced_at\n    args_mapping: 'root = [this.id, this.name, this.price, this.synced_at]'"
  },
  {
    "id": "sqs-to-sns-fanout",
    "name": "AWS SQS to SNS Fanout",
    "description": "Fan out SQS messages to multiple SNS topics",
    "keywords": [
      "sqs",
      "sns",
      "aws",
      "fanout",
      "pubsub",
      "broadcast",
      "multiple",
      "topics"
    ],
    "components": {
      "inputs": [
        "aws_sqs"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "broker"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  aws_sqs:\n    url: https://sqs.us-east-1.amazonaws.com/123456789/incoming\n    region: us-east-1\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.distributed_at = now()\n\noutput:\n  broker:\n    pattern: fan_out\n    outputs:\n      - aws_sns:\n          topic_arn: arn:aws:sns:us-east-1:123456789:notifications\n          region: us-east-1\n      - aws_sns:\n          topic_arn: arn:aws:sns:us-east-1:123456789:analytics\n          region: us-east-1\n      - aws_sns:\n          topic_arn: arn:aws:sns:us-east-1:123456789:archival\n          region: us-east-1"
  },
  {
    "id": "state-machine-mapping",
    "name": "State Machine Mapping (Weihenstephaner)",
    "description": "Map machine states to standardized state codes",
    "keywords": [
      "state",
      "machine",
      "weihenstephaner",
      "packml",
      "mapping",
      "oee"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "match expression",
      "exists()",
      "deleted()"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics: [machine-state]\n\npipeline:\n  processors:\n    - mapping: |\n        if this.exists(\"machine_state_code\") {\n          root = {\n            \"start_time_unix_ms\": this.timestamp_ms,\n            \"state\": match this.machine_state_code {\n              0 => 30000,\n              1 => 40000,\n              2 => 20000,\n              4 => 40000,\n              8 => 60000,\n              16 => 70000,\n              32 => 80000,\n              64 => 80000,\n              128 => 10000,\n              _ => 0\n            }\n          }\n        } else {\n          root = deleted()\n        }\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: standardized-state"
  },
  {
    "id": "stdin-transform-stdout",
    "name": "Stdin to Stdout Transform",
    "description": "Read from stdin, transform, and write to stdout",
    "keywords": [
      "stdin",
      "stdout",
      "transform",
      "cli",
      "pipe",
      "test"
    ],
    "components": {
      "inputs": [
        "stdin"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "now()"
    ],
    "yaml": "input:\n  stdin: {}\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.processed_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "string-cleaning",
    "name": "String Cleaning and Sanitization",
    "description": "Remove specific characters from strings using replace_all",
    "keywords": [
      "string",
      "clean",
      "sanitize",
      "replace",
      "remove",
      "characters"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "replace_all()"
    ],
    "yaml": "input:\n  kafka:\n    addresses: [localhost:9092]\n    topics: [raw-strings]\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.cleaned_string = this.raw_string.replace_all(\"-\", \"\").replace_all(\"_\", \"\")\n        root.topic_clean = this.topic.replace_all(\".\", \"_\")\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: cleaned-data"
  },
  {
    "id": "tcp-network-device",
    "name": "TCP Network Device Communication",
    "description": "Communicate with network devices (scales, sensors) via TCP",
    "keywords": [
      "tcp",
      "network",
      "device",
      "scale",
      "sensor",
      "netcat",
      "command"
    ],
    "components": {
      "inputs": [
        "generate"
      ],
      "processors": [
        "command",
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "content()",
      "starts_with()",
      "trim_prefix()",
      "trim_suffix()",
      "re_replace_all()"
    ],
    "yaml": "input:\n  generate:\n    interval: 1s\n    mapping: |\n      root = \"SI\\r\\n\"\n\npipeline:\n  processors:\n    - command:\n        name: nc\n        args_mapping: '[\"10.117.216.80\", \"8000\"]'\n    - mapping: |\n        if content().string().starts_with(\"SS\") {\n          let weight = content()\n            .replace_all(\" \", \"\")\n            .trim_prefix(\"SS\")\n            .trim_suffix(\"g\\r\\n\")\n            .number()\n          let unit = content().string().re_replace_all(\"[^a-zA-Z]\", \"\")\n          root = {\n            \"value\": $weight,\n            \"timestamp_ms\": (timestamp_unix_nano() / 1000000).floor(),\n            \"unit\": $unit\n          }\n        } else {\n          root = deleted()\n        }\n\noutput:\n  kafka:\n    addresses: [localhost:9092]\n    topic: scale-readings"
  },
  {
    "id": "time-window-grouping",
    "name": "Time Window Grouping",
    "description": "Group messages by time windows",
    "keywords": [
      "time",
      "window",
      "grouping",
      "batch",
      "aggregate",
      "interval"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "aws_s3"
      ]
    },
    "bloblangPatterns": [
      "uuid_v4()",
      "map_each()",
      "parse_json()",
      "length()",
      "now()",
      "format_timestamp()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n    batching:\n      period: 60s\n      count: 1000\n\npipeline:\n  processors:\n    - mapping: |\n        root.window_id = uuid_v4()\n        root.events = this.map_each(e -> e.parse_json())\n        root.count = this.length()\n        root.window_end = now()\n\noutput:\n  aws_s3:\n    bucket: time-windows\n    path: ${! now().format_timestamp(\"2006/01/02/15\") }/${! this.window_id }.json"
  },
  {
    "id": "try-catch-fallback",
    "name": "Try-Catch with Fallback Processing",
    "description": "Handle errors with fallback logic using try/catch",
    "keywords": [
      "try",
      "catch",
      "fallback",
      "error",
      "handling",
      "graceful",
      "degradation",
      "recovery"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "try",
        "catch",
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "error()",
      "now()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n\npipeline:\n  processors:\n    - try:\n        - mapping: |\n            root = this.parse_json()\n            root.validated = true\n    - catch:\n        - mapping: |\n            # Fallback: keep original as string if JSON parsing fails\n            root.raw_data = this\n            root.validated = false\n            root.parse_error = error()\n    - mapping: |\n        root.processed_at = now()\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: validated-events"
  },
  {
    "id": "validation-rejection",
    "name": "Message Validation with Rejection",
    "description": "Validate messages and reject malformed ones",
    "keywords": [
      "validate",
      "validation",
      "reject",
      "schema",
      "check",
      "filter",
      "malformed",
      "quality"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "switch"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "let",
      "if { } else { }",
      "meta"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - incoming\n\npipeline:\n  processors:\n    - mapping: |\n        let data = this.parse_json()\n\n        # Validate required fields\n        let valid = $data.id != null && $data.type != null && $data.payload != null\n\n        root = $data\n        meta valid = if $valid { \"true\" } else { \"false\" }\n        meta rejection_reason = if !$valid {\n          if $data.id == null { \"missing id\" }\n          else if $data.type == null { \"missing type\" }\n          else { \"missing payload\" }\n        } else { \"\" }\n\noutput:\n  switch:\n    cases:\n      - check: meta(\"valid\") == \"true\"\n        output:\n          kafka:\n            addresses:\n              - localhost:9092\n            topic: valid-messages\n      - output:\n          kafka:\n            addresses:\n              - localhost:9092\n            topic: rejected-messages"
  },
  {
    "id": "webhook-handler",
    "name": "Webhook Event Handler",
    "description": "Receive webhook events and process them",
    "keywords": [
      "webhook",
      "event",
      "handler",
      "http",
      "server",
      "callback"
    ],
    "components": {
      "inputs": [
        "http_server"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "uuid_v4()",
      "now()"
    ],
    "yaml": "input:\n  http_server:\n    address: 0.0.0.0:8080\n    path: /webhook\n    allowed_verbs:\n      - POST\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_json()\n        root.event_id = uuid_v4()\n        root.received_at = now()\n\noutput:\n  stdout: {}"
  },
  {
    "id": "webhook-to-kafka",
    "name": "HTTP Webhook to Kafka",
    "description": "Receive HTTP webhooks and forward to Kafka",
    "keywords": [
      "http",
      "webhook",
      "kafka",
      "api",
      "ingest",
      "rest"
    ],
    "components": {
      "inputs": [
        "http_server"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "now()"
    ],
    "yaml": "input:\n  http_server:\n    address: 0.0.0.0:8080\n    path: /webhook\n    allowed_verbs:\n      - POST\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        root.received_at = now()\n        root.source = \"webhook\"\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: webhook-events"
  },
  {
    "id": "windowed-aggregation",
    "name": "Windowed Aggregation",
    "description": "Aggregate data in time-based windows",
    "keywords": [
      "windowed",
      "window",
      "aggregation",
      "aggregate",
      "time",
      "batch"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "aws_s3"
      ]
    },
    "bloblangPatterns": [
      "uuid_v4()",
      "map_each()",
      "parse_json()",
      "length()",
      "now()",
      "format_timestamp()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - events\n    batching:\n      period: 60s\n      count: 1000\n\npipeline:\n  processors:\n    - mapping: |\n        root.window_id = uuid_v4()\n        root.events = this.map_each(e -> e.parse_json())\n        root.count = this.length()\n        root.window_end = now()\n\noutput:\n  aws_s3:\n    bucket: windowed-data\n    path: ${! now().format_timestamp(\"2006/01/02/15\") }/${! this.window_id }.json"
  },
  {
    "id": "workflow-dag",
    "name": "Workflow DAG Processing",
    "description": "Process messages through a directed acyclic graph of stages",
    "keywords": [
      "workflow",
      "dag",
      "pipeline",
      "stages",
      "dependencies",
      "orchestration",
      "graph"
    ],
    "components": {
      "inputs": [
        "kafka"
      ],
      "processors": [
        "workflow",
        "mapping"
      ],
      "outputs": [
        "kafka"
      ]
    },
    "bloblangPatterns": [
      "parse_json()",
      "merge()"
    ],
    "yaml": "input:\n  kafka:\n    addresses:\n      - localhost:9092\n    topics:\n      - raw-data\n\npipeline:\n  processors:\n    - workflow:\n        meta_path: workflow_status\n        order:\n          - - parse\n          - - validate\n            - enrich\n          - - transform\n        branches:\n          parse:\n            request_map: 'root = this'\n            processors:\n              - mapping: 'root = this.parse_json()'\n            result_map: 'root.parsed = this'\n          validate:\n            request_map: 'root = this.parsed'\n            processors:\n              - mapping: 'root.valid = this.id != null'\n            result_map: 'root.validation = this'\n          enrich:\n            request_map: 'root = this.parsed'\n            processors:\n              - mapping: 'root.enriched = true'\n            result_map: 'root.enrichment = this'\n          transform:\n            request_map: 'root = this'\n            processors:\n              - mapping: |\n                  root = this.parsed\n                  root.metadata = this.validation.merge(this.enrichment)\n            result_map: 'root = this'\n\noutput:\n  kafka:\n    addresses:\n      - localhost:9092\n    topic: processed-data"
  },
  {
    "id": "xml-to-json",
    "name": "XML to JSON Pipeline",
    "description": "Convert XML data to JSON format",
    "keywords": [
      "xml",
      "json",
      "convert",
      "transform",
      "parse",
      "format"
    ],
    "components": {
      "inputs": [
        "file"
      ],
      "processors": [
        "mapping"
      ],
      "outputs": [
        "stdout"
      ]
    },
    "bloblangPatterns": [
      "parse_xml()",
      "now()"
    ],
    "yaml": "input:\n  file:\n    paths:\n      - /data/*.xml\n\npipeline:\n  processors:\n    - mapping: |\n        root = this.parse_xml()\n        root.converted_at = now()\n\noutput:\n  stdout: {}"
  }
];
